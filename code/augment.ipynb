{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scaper\n",
    "import random\n",
    "%cd /nfs/home/pedro.lopes/data/audio_data/train\n",
    "import audiofile as af\n",
    "import IPython.display as ipd\n",
    "from glob import glob\n",
    "from os import path\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sampling_rate = af.read('/nfs/home/pedro.lopes/data/audio_data/train/augmented/val/sample_299998_vocals.wav')\n",
    "ipd.Audio(signal,rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_encode = tf.audio.encode_wav(signal,sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.write_file(\n",
    "    '/nfs/home/pedro.lopes/data/test_audio_tf.wav', wav_encode, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = tf.io.read_file(\n",
    "    '/nfs/home/pedro.lopes/data/test_audio_tf.wav', name=None\n",
    ")\n",
    "audio = tf.audio.decode_wav(buf)\n",
    "audio[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(np.squeeze(audio[0].numpy()),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_data = tf.signal.stft(signal, \n",
    "\tframe_length=1024, \n",
    "\tframe_step=512,\n",
    "\tfft_length=1024).numpy().T\n",
    "\n",
    "json_path = '/home/pedro.lopes/pf/norm_data_full.json'\n",
    "with open(json_path) as infile:\n",
    "    norm_data = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_zeros = int(np.ceil(x.shape[-1]/128)*128 - x.shape[-1])\n",
    "pad = ([0,0],[0,num_zeros])\n",
    "x = np.pad(x,pad,mode='constant', constant_values=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sampling_rate = af.read('/nfs/home/pedro.lopes/data/audio_data/train/augmented/train/sample_50_mix.wav')\n",
    "signal = np.array([signal,signal])\n",
    "stft_data = tf.signal.stft(signal, \n",
    "\tframe_length=1024, \n",
    "\tframe_step=512,\n",
    "\tfft_length=1024)\n",
    "x_before = tf.keras.backend.permute_dimensions(stft_data,(0,2,1))\n",
    "x_shape = tf.cast(tf.shape(x_before)[-1],tf.float64)\n",
    "pad_len = tf.math.floor(tf.math.ceil(x_shape/128)*128 - x_shape)\n",
    "pad = ([0,0],[0,0],[0,pad_len])\n",
    "x = tf.pad(x_before,pad,mode='constant', constant_values=0)\n",
    "X_mean = norm_data['X_min']\n",
    "X_std = norm_data['X_max'] - norm_data['X_min']\n",
    "def normalize(x,save=False):\n",
    "    scaled_x = (x - np.mean(x))/(np.abs(np.std(x))+1e-8)\n",
    "\n",
    "    if save:\n",
    "      return scaled_x, np.mean(x), np.std(x)\n",
    "    return scaled_x\n",
    "def normalize_from_outer(x,x_mean,x_std):\n",
    "  scaled_x = (x - x_mean)/(x_std+1e-8)\n",
    "  return scaled_x\n",
    "def preprocess(sample,x_mean,x_std):\n",
    "  log_sample = np.log(np.abs(sample)+1e-7)\n",
    "  mod_input = normalize_from_outer(log_sample,x_mean,x_std)\n",
    "  return mod_input\n",
    "\n",
    "def preprocess_tf(sample,x_mean,x_std):\n",
    "  log_sample = tf.math.log(tf.math.abs(sample)+1e-7)\n",
    "  mod_input = normalize_from_outer(log_sample,x_mean,x_std)\n",
    "  return mod_input\n",
    "x = preprocess_tf(x,X_mean,X_std)[:,0:512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_split = tf.split(x,7,axis = -1)\n",
    "tf.concat(x_split,axis=0)\n",
    "#tf.keras.backend.transpose(x_split[:,0,:,0]) - x[0,:,0:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.repeat([128],x_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"/home/pedro.lopes/data/audio_data/train/augmented/train/*\"\n",
    "names = [path.basename(x) for x in glob(pattern)]\n",
    "%cd /home/pedro.lopes/pf/code/\n",
    "names_corrected =[]\n",
    "for name in names:\n",
    "    splitted = 'train/' + name.rsplit('_',maxsplit=1)[0]\n",
    "    names_corrected.append(splitted)\n",
    "training_split = list(dict.fromkeys(names_corrected))\n",
    "\n",
    "pattern = \"/home/pedro.lopes/data/audio_data/train/augmented/val/*\"\n",
    "names = [path.basename(x) for x in glob(pattern)]\n",
    "%cd /home/pedro.lopes/pf/code/\n",
    "names_corrected =[]\n",
    "for name in names:\n",
    "    splitted = 'val/' + name.rsplit('_',maxsplit=1)[0]\n",
    "    names_corrected.append(splitted)\n",
    "validation_split = list(dict.fromkeys(names_corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/pedro.lopes/data/audio_data/train/augmented/'\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "def get_dataset(filepaths):\n",
    "    vocal_filepaths = [dataset_path + x + '_vocals.wav' for x in filepaths]\n",
    "    labels = [1 for x in filepaths]\n",
    "    mix_filepaths = [dataset_path + x + '_mix.wav' for x in filepaths]\n",
    "    X_ds = tf.data.Dataset.from_tensor_slices(mix_filepaths)\n",
    "    y_ds = tf.data.Dataset.from_tensor_slices(labels)#tf.data.Dataset.from_tensor_slices(vocal_filepaths)\n",
    "    return tf.data.Dataset.zip((X_ds, y_ds))\n",
    "\n",
    "\n",
    "def load_audio(filepath, label):\n",
    "    # Load 10 seconds of audio at 44.1kHz sample-rate\n",
    "    audio = tf.io.read_file(filepath)\n",
    "    audio, sample_rate = tf.audio.decode_wav(audio,\n",
    "                                             desired_channels=-1,\n",
    "                                             desired_samples=-1)\n",
    "    return audio, label\n",
    "\n",
    "\n",
    "def prepare_for_training(ds, shuffle_buffer_size=1024, batch_size=16):\n",
    "    # Randomly shuffle (file_path, label) dataset\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    # Load and decode audio from file paths\n",
    "    ds = ds.map(load_audio, num_parallel_calls=AUTOTUNE)\n",
    "    # Repeat dataset forever\n",
    "    ds = ds.repeat()\n",
    "    # Prepare batches\n",
    "    ds = ds.batch(batch_size)\n",
    "    # Prefetch\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))    \n",
    "with strategy.scope():\n",
    "    ds = get_dataset(training_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.map(load_audio).take(10).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = list(range(1,1335,1))\n",
    "validation_size = 0.2\n",
    "validation_items = random.sample(numbers,k=int(len(numbers)*validation_size))\n",
    "training_items = [x for x in numbers if x not in validation_items]\n",
    "training_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_items.sort()\n",
    "validation_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in training_items:\n",
    "    name = 'sample_' + str(i) + '.wav'\n",
    "    vocal_src = 'vocals/' + name\n",
    "    vocal_dest = 'train/vocals/' + name\n",
    "    acc_src = 'acc/' + name\n",
    "    acc_dest = 'train/acc/' + name\n",
    "    mix_src = 'mix/' + name\n",
    "    mix_dest = 'train/mix/' + name\n",
    "    !mv $vocal_src $vocal_dest\n",
    "    !mv $acc_src $acc_dest\n",
    "    !mv $mix_src $mix_dest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in validation_items:\n",
    "    name = 'sample_' + str(i) + '.wav'\n",
    "    vocal_src = 'vocals/' + name\n",
    "    vocal_dest = 'val/vocals/' + name\n",
    "    acc_src = 'acc/' + name\n",
    "    acc_dest = 'val/acc/' + name\n",
    "    mix_src = 'mix/' + name\n",
    "    mix_dest = 'val/mix/' + name\n",
    "    print(vocal_src + ' // ' + vocal_dest)\n",
    "    !mv $vocal_src $vocal_dest\n",
    "    !mv $acc_src $acc_dest\n",
    "    !mv $mix_src $mix_dest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_state = 123\n",
    "# create foreground folder\n",
    "fg_folder = Path('~/data/audio_data/train/train').expanduser()  \n",
    "fg_folder.mkdir(parents=True, exist_ok=True)                             \n",
    "\n",
    "# create background folder - we need to provide one even if we don't use it\n",
    "bg_folder = Path('~/data/audio_data/train/empty').expanduser()\n",
    "\n",
    "labels = ['vocals', 'acc']\n",
    "# 2. Create a Scaper object\n",
    "sc = scaper.Scaper(\n",
    "    duration=5.0,\n",
    "    fg_path=str(fg_folder),\n",
    "    bg_path=str(bg_folder),\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# 3. Set sample rate, reference dB, and channels (mono)\n",
    "sc.sr = 44100\n",
    "sc.ref_db = -20\n",
    "sc.n_channels = 1\n",
    "\n",
    "# 4. Define a template of probabilistic event parameters\n",
    "event_parameters= {\n",
    "    'label': ('const', 'vocals'),\n",
    "    'source_file': ('choose', []),\n",
    "    'source_time': ('uniform', 0, 1000),\n",
    "    'event_time': ('const', 0),\n",
    "    'event_duration': ('const', 10.0),\n",
    "    'snr': ('uniform', -5, 5),\n",
    "    'pitch_shift': ('uniform', -2, 2),\n",
    "    'time_stretch': ('uniform', 0.8, 1.2)\n",
    "}\n",
    "\n",
    "# 5. Instatiate the template once to randomly choose a song,\n",
    "#    a start time for the sources, a pitch shift and a time\n",
    "#    stretch. These values must remain COHERENT across all stems\n",
    "   \n",
    "# Add a an event based on the probabilistic template\n",
    "sc.add_event(**event_parameters)\n",
    "\n",
    "# Instantiate the event to sample concrete values\n",
    "event = sc._instantiate_event(sc.fg_spec[0])\n",
    "    \n",
    "# 6. Reset the Scaper object's event specficiation\n",
    "sc.reset_fg_event_spec()\n",
    "    \n",
    "# 7. Replace the distributions for source time, pitch shift and\n",
    "#    time stretch with the constant values we just sampled, to \n",
    "#    ensure our added events (stems) are coherent.    \n",
    "#    NOTE: the source_file has also been sampled, and we'll keep\n",
    "#    the sampled file to denote which song we'll be mixing.\n",
    "event_parameters['source_time'] = ('const', event.source_time)\n",
    "event_parameters['pitch_shift'] = ('const', event.pitch_shift)\n",
    "event_parameters['time_stretch'] = ('const', event.time_stretch)\n",
    "\n",
    "# 8. Iterate over the four stems (vocals, drums, bass, other) and \n",
    "#    add COHERENT events.\n",
    "labels = labels\n",
    "\n",
    "for label in labels:\n",
    "\n",
    "    # Set the label to the stem we are adding\n",
    "    event_parameters['label'] = ('const', label)\n",
    "\n",
    "    # To ensure coherent source files (all from the same song), we leverage\n",
    "    # the fact that all the stems from the same song have the same filename.\n",
    "    # All we have to do is replace the stem file's parent folder name from \"vocals\" \n",
    "    # to the label we are adding in this iteration of the loop, which will give the \n",
    "    # correct path to the stem source file for this current label.\n",
    "    coherent_source_file = event.source_file.replace('vocals', label)\n",
    "    event_parameters['source_file'] = ('const', coherent_source_file)\n",
    "\n",
    "    # Add the event using the modified, COHERENT, event parameters\n",
    "    sc.add_event(**event_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_play(sc):\n",
    "\n",
    "    mixture_audio, mixture_jam, annotation_list, stem_audio_list = sc.generate(fix_clipping=True)\n",
    "\n",
    "    print(\"Mixture:\")\n",
    "    display(Audio(data=mixture_audio.T, rate=sc.sr))\n",
    "\n",
    "    # extract the annotation data from the JAMS object\n",
    "    ann = mixture_jam.annotations.search(namespace='scaper')[0]\n",
    "    \n",
    "    # iterate over the annotation and corresponding stem audio data\n",
    "    for obs, stem_audio in zip(ann.data, stem_audio_list):\n",
    "        print(f\"Instrument: {obs.value['label']} at SNR: {obs.value['snr']:.2f}\")\n",
    "        display(Audio(data=stem_audio.T, rate=sc.sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_play(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_event_parameters = {\n",
    "    'label': ('const', 'vocals'),\n",
    "    'source_file': ('choose', []),\n",
    "    'source_time': ('uniform', 0, 1000),\n",
    "    'event_time': ('const', 0),\n",
    "    'event_duration': ('const', 10.0),\n",
    "    'snr': ('uniform', -5, 5),\n",
    "    'pitch_shift': ('uniform', -2, 2),\n",
    "    'time_stretch': ('uniform', 0.8, 1.2)\n",
    "}\n",
    "\n",
    "def incoherent(fg_folder, bg_folder, event_template, seed):\n",
    "    \"\"\"\n",
    "    This function takes the paths to the MUSDB18 source materials, an event template, \n",
    "    and a random seed, and returns an INCOHERENT mixture (audio + annotations). \n",
    "    \n",
    "    Stems in INCOHERENT mixtures may come from different songs and are not temporally\n",
    "    aligned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fg_folder : str\n",
    "        Path to the foreground source material for MUSDB18\n",
    "    bg_folder : str\n",
    "        Path to the background material for MUSDB18 (empty folder)\n",
    "    event_template: dict\n",
    "        Dictionary containing a template of probabilistic event parameters\n",
    "    seed : int or np.random.RandomState()\n",
    "        Seed for setting the Scaper object's random state. Different seeds will \n",
    "        generate different mixtures for the same source material and event template.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mixture_audio : np.ndarray\n",
    "        Audio signal for the mixture\n",
    "    mixture_jams : np.ndarray\n",
    "        JAMS annotation for the mixture\n",
    "    annotation_list : list\n",
    "        Simple annotation in list format\n",
    "    stem_audio_list : list\n",
    "        List containing the audio signals of the stems that comprise the mixture\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create scaper object and seed random state\n",
    "    sc = scaper.Scaper(\n",
    "        duration=10.0,\n",
    "        fg_path=str(fg_folder),\n",
    "        bg_path=str(bg_folder),\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Set sample rate, reference dB, and channels (mono)\n",
    "    sc.sr = 44100\n",
    "    sc.ref_db = -20\n",
    "    sc.n_channels = 1\n",
    "    \n",
    "    # Copy the template so we can change it\n",
    "    event_parameters = event_template.copy()\n",
    "    \n",
    "    # Iterate over stem types and add INCOHERENT events\n",
    "    labels = ['vocals', 'acc']\n",
    "    for label in labels:\n",
    "        event_parameters['label'] = ('const', label)\n",
    "        sc.add_event(**event_parameters)\n",
    "    \n",
    "    # Return the generated mixture audio + annotations \n",
    "    # while ensuring we prevent audio clipping\n",
    "    return sc.generate(fix_clipping=False)\n",
    "\n",
    "\n",
    "def coherent(fg_folder, bg_folder, event_template, seed):\n",
    "    \"\"\"\n",
    "    This function takes the paths to the MUSDB18 source materials and a random seed,\n",
    "    and returns an COHERENT mixture (audio + annotations).\n",
    "    \n",
    "    Stems in COHERENT mixtures come from the same song and are temporally aligned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fg_folder : str\n",
    "        Path to the foreground source material for MUSDB18\n",
    "    bg_folder : str\n",
    "        Path to the background material for MUSDB18 (empty folder)\n",
    "    event_template: dict\n",
    "        Dictionary containing a template of probabilistic event parameters\n",
    "    seed : int or np.random.RandomState()\n",
    "        Seed for setting the Scaper object's random state. Different seeds will \n",
    "        generate different mixtures for the same source material and event template.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mixture_audio : np.ndarray\n",
    "        Audio signal for the mixture\n",
    "    mixture_jams : np.ndarray\n",
    "        JAMS annotation for the mixture\n",
    "    annotation_list : list\n",
    "        Simple annotation in list format\n",
    "    stem_audio_list : list\n",
    "        List containing the audio signals of the stems that comprise the mixture\n",
    "    \"\"\"\n",
    "        \n",
    "    # Create scaper object and seed random state\n",
    "    sc = scaper.Scaper(\n",
    "        duration=10.0,\n",
    "        fg_path=str(fg_folder),\n",
    "        bg_path=str(bg_folder),\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Set sample rate, reference dB, and channels (mono)\n",
    "    sc.sr = 44100\n",
    "    sc.ref_db = -20\n",
    "    sc.n_channels = 1\n",
    "    \n",
    "    # Copy the template so we can change it\n",
    "    event_parameters = event_template.copy()    \n",
    "    \n",
    "    # Instatiate the template once to randomly choose a song,   \n",
    "    # a start time for the sources, a pitch shift and a time    \n",
    "    # stretch. These values must remain COHERENT across all stems\n",
    "    sc.add_event(**event_parameters)\n",
    "    event = sc._instantiate_event(sc.fg_spec[0])\n",
    "    \n",
    "    # Reset the Scaper object's the event specification\n",
    "    sc.reset_fg_event_spec()\n",
    "    \n",
    "    # Replace the distributions for source time, pitch shift and \n",
    "    # time stretch with the constant values we just sampled, to  \n",
    "    # ensure our added events (stems) are coherent.              \n",
    "    event_parameters['source_time'] = ('const', event.source_time)\n",
    "    event_parameters['pitch_shift'] = ('const', event.pitch_shift)\n",
    "    event_parameters['time_stretch'] = ('const', event.time_stretch)\n",
    "\n",
    "    # Iterate over the four stems (vocals, drums, bass, ot) and \n",
    "    # add COHERENT events.                                         \n",
    "    labels = ['vocals', 'acc']\n",
    "    for label in labels:\n",
    "        \n",
    "        # Set the label to the stem we are adding\n",
    "        event_parameters['label'] = ('const', label)\n",
    "        \n",
    "        # To ensure coherent source files (all from the same song), we leverage\n",
    "        # the fact that all the stems from the same song have the same filename.\n",
    "        # All we have to do is replace the stem file's parent folder name from \"vocals\" \n",
    "        # to the label we are adding in this iteration of the loop, which will give the \n",
    "        # correct path to the stem source file for this current label.\n",
    "        coherent_source_file = event.source_file.replace('vocals', label)\n",
    "        event_parameters['source_file'] = ('const', coherent_source_file)\n",
    "        # Add the event using the modified, COHERENT, event parameters\n",
    "        sc.add_event(**event_parameters)\n",
    "    \n",
    "    # Generate and return the mixture audio, stem audio, and annotations\n",
    "    return sc.generate(fix_clipping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fg_folder = Path('~/data/audio_data/train/val').expanduser() \n",
    "for seed in [np.random.randint(0,100000)]:\n",
    "    \n",
    "    mixture_audio, mixture_jam, annotation_list, stem_audio_list = coherent(\n",
    "        fg_folder, \n",
    "        bg_folder, \n",
    "        template_event_parameters, \n",
    "        seed)\n",
    "    ann = mixture_jam.annotations.search(namespace='scaper')[0]\n",
    "\n",
    "    for obs, stem_audio in zip(ann.data, stem_audio_list):\n",
    "        print(f\"Instrument: {obs.value['label']} at SNR: {obs.value['snr']:.2f}\")\n",
    "        if obs.value['label'] == 'vocals':\n",
    "            vocals = stem_audio\n",
    "        if obs.value['label'] == 'acc':\n",
    "            acc = stem_audio\n",
    "vocals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "bg_folder = Path('~/data/audio_data/train/empty').expanduser()\n",
    "augmented_path = '/nfs/home/pedro.lopes/data/audio_data/train/augmented/'\n",
    "SAMPLE_RATE=44100\n",
    "start = 0\n",
    "N_total = 300000\n",
    "validation_size = 0.1\n",
    "N_val = int(validation_size*(N_total-start))\n",
    "N_train = N_total - (N_val + start)\n",
    "coherent_p = 0.8\n",
    "training_range = range(start,start + N_train)\n",
    "validation_range = range(start+N_train,N_total,1)\n",
    "\n",
    "fg_folder = Path('~/data/audio_data/train/train/clean').expanduser()  \n",
    "fg_folder.mkdir(parents=True, exist_ok=True)   \n",
    "# for i in tqdm(training_range):\n",
    "#     seed = np.random.randint(0,1000000)\n",
    "#     if np.random.uniform()<=coherent_p:\n",
    "#         mixture_audio, mixture_jam, annotation_list, stem_audio_list = coherent(\n",
    "#             fg_folder, \n",
    "#             bg_folder, \n",
    "#             template_event_parameters, \n",
    "#             seed)\n",
    "#     else:\n",
    "#         mixture_audio, mixture_jam, annotation_list, stem_audio_list = incoherent(\n",
    "#             fg_folder, \n",
    "#             bg_folder, \n",
    "#             template_event_parameters, \n",
    "#             seed)\n",
    "#     ann = mixture_jam.annotations.search(namespace='scaper')[0]\n",
    "#     for obs, stem_audio in zip(ann.data, stem_audio_list):\n",
    "#         if obs.value['label'] == 'vocals':\n",
    "#             vocals = stem_audio\n",
    "            \n",
    "#     vocal_wav_encode = tf.audio.encode_wav(vocals,SAMPLE_RATE)\n",
    "#     tf.io.write_file(\n",
    "#         augmented_path + 'train/sample_' + str(i) + '_vocals.wav', vocal_wav_encode, name=None\n",
    "#     )\n",
    "#     mix_wav_encode = tf.audio.encode_wav(mixture_audio,SAMPLE_RATE)\n",
    "#     tf.io.write_file(\n",
    "#         augmented_path + 'train/sample_' + str(i) + '_mix.wav', mix_wav_encode, name=None\n",
    "#     )\n",
    "#     #librosa.output.write_wav(augmented_path + 'train/sample_' + str(i) + '_mix.wav',y=mixture_audio,sr=SAMPLE_RATE)\n",
    "#     #librosa.output.write_wav(augmented_path + 'train/sample_' + str(i) + '_vocals.wav',y=vocals.T,sr=SAMPLE_RATE)\n",
    "    \n",
    "fg_folder = Path('~/data/audio_data/train/val/clean').expanduser()  \n",
    "fg_folder.mkdir(parents=True, exist_ok=True)                             \n",
    "for i in tqdm(validation_range):\n",
    "    seed = np.random.randint(0,1000000)\n",
    "    if np.random.uniform()<=coherent_p:\n",
    "        mixture_audio, mixture_jam, annotation_list, stem_audio_list = coherent(\n",
    "            fg_folder, \n",
    "            bg_folder, \n",
    "            template_event_parameters, \n",
    "            seed)\n",
    "    else:\n",
    "        mixture_audio, mixture_jam, annotation_list, stem_audio_list = incoherent(\n",
    "            fg_folder, \n",
    "            bg_folder, \n",
    "            template_event_parameters, \n",
    "            seed)\n",
    "    ann = mixture_jam.annotations.search(namespace='scaper')[0]\n",
    "    for obs, stem_audio in zip(ann.data, stem_audio_list):\n",
    "        if obs.value['label'] == 'vocals':\n",
    "            vocals = stem_audio\n",
    "    vocal_wav_encode = tf.audio.encode_wav(vocals,SAMPLE_RATE)\n",
    "    tf.io.write_file(\n",
    "        augmented_path + 'val/sample_' + str(i) + '_vocals.wav', vocal_wav_encode, name=None\n",
    "    )\n",
    "    mix_wav_encode = tf.audio.encode_wav(mixture_audio,SAMPLE_RATE)\n",
    "    tf.io.write_file(\n",
    "        augmented_path + 'val/sample_' + str(i) + '_mix.wav', mix_wav_encode, name=None\n",
    "    )\n",
    "#     librosa.output.write_wav(augmented_path + 'val/sample_' + str(i) + '_mix.wav',y=mixture_audio,sr=SAMPLE_RATE)\n",
    "#     librosa.output.write_wav(augmented_path + 'val/sample_' + str(i) + '_vocals.wav',y=vocals.T,sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "i=3002\n",
    "y,sr = librosa.core.load(augmented_path + 'train/sample_' + str(i) + '_vocals.wav',sr=None)\n",
    "Audio(y,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import coherent\n",
    "mixture_audio, mixture_jam, annotation_list, stem_audio_list = coherent(\n",
    "            fg_folder, \n",
    "            bg_folder, \n",
    "            template_event_parameters, \n",
    "            seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total = 50000\n",
    "num_in_dir = 100\n",
    "validation_size = 0.1\n",
    "N_val = int(validation_size*N_total)\n",
    "N_train = N_total - N_val\n",
    "coherent_p = 0.8\n",
    "training_range = range(0,N_train)\n",
    "validation_range = range(N_train,N_total,1)\n",
    "%cd /nfs/home/pedro.lopes/data/audio_data/train/augmented/train\n",
    "curr_dir = 0\n",
    "sample_num = 0\n",
    "num_dir = int(N_train/num_in_dir)\n",
    "for curr_dir in tqdm(range(1,num_dir+1)):\n",
    "    if not(path.isdir(str(curr_dir))):\n",
    "        dir_name = str(curr_dir) + '/'\n",
    "        !mkdir $dir_name\n",
    "    for i in range(num_in_dir):\n",
    "        sample_num+=1\n",
    "        orig_path_mix = 'sample_' + str(sample_num) + '_mix.wav'\n",
    "        orig_path_vocals = 'sample_' + str(sample_num) + '_vocals.wav'\n",
    "        dest_path_mix = str(curr_dir) + '/sample_' + str(sample_num) + '_mix.wav'\n",
    "        dest_path_vocals = str(curr_dir) + '/sample_' + str(sample_num) + '_vocals.wav'\n",
    "        !mv $orig_path_mix $dest_path_mix\n",
    "        !mv $orig_path_vocals $dest_path_vocals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total = 50000\n",
    "num_in_dir = 100\n",
    "validation_size = 0.1\n",
    "N_val = int(validation_size*N_total)\n",
    "N_train = N_total - N_val\n",
    "coherent_p = 0.8\n",
    "training_range = range(0,N_train)\n",
    "validation_range = range(N_train,N_total,1)\n",
    "%cd /nfs/home/pedro.lopes/data/audio_data/train/augmented/val\n",
    "curr_dir = 0\n",
    "sample_num = 45000\n",
    "num_dir = int(N_val/num_in_dir)\n",
    "while tqdm(curr_dir < num_dir):\n",
    "    curr_dir+=1\n",
    "    if not(path.isdir(curr_dir)):\n",
    "        dir_name = str(curr_dir) + '/'\n",
    "        !mkdir $dir_name\n",
    "    for i in range(num_in_dir):\n",
    "        orig_path_mix = 'sample_' + str(sample_num) + '_mix.wav'\n",
    "        orig_path_vocals = 'sample_' + str(sample_num) + '_vocals.wav'\n",
    "        dest_path_mix = str(curr_dir) + '/sample_' + str(sample_num) + '_mix.wav'\n",
    "        dest_path_vocals = str(curr_dir) + '/sample_' + str(sample_num) + '_vocals.wav'\n",
    "        !mv $orig_path_mix $dest_path_mix\n",
    "        !mv $orig_path_vocals $dest_path_vocals\n",
    "        sample_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "pattern = \"/home/pedro.lopes/data/audio_data/train/augmented/train/*/*/*\"\n",
    "names = glob(pattern)\n",
    "names_corrected =[]\n",
    "for name in names:\n",
    "    splitted = name.rsplit('_',maxsplit=1)[0]\n",
    "    names_corrected.append(splitted)\n",
    "training_range = list(dict.fromkeys(names_corrected))\n",
    "\n",
    "pattern = \"/home/pedro.lopes/data/audio_data/train/augmented/val/*/*/*\"\n",
    "names = glob(pattern)\n",
    "names_corrected =[]\n",
    "for name in names:\n",
    "    splitted = name.rsplit('_',maxsplit=1)[0]\n",
    "    names_corrected.append(splitted)\n",
    "validation_range = list(dict.fromkeys(names_corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"/home/pedro.lopes/data/audio_data/train/train/vocals/*\"\n",
    "training_range = glob(pattern)\n",
    "training_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio = af.read(training_range[500] + '_vocals.wav')\n",
    "ipd.Audio(audio[0],rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"/home/pedro.lopes/data/audio_data/train/val/mix/*\"\n",
    "training_range = glob(pattern)\n",
    "mir_1k_items = ['sample_' + str(x) + '.wav' for x in range(150,1150,1)]\n",
    "train_clean = [x for x in training_range if x.rsplit('/')[-1] not in mir_1k_items]\n",
    "for item in train_clean:\n",
    "    dest = \"/home/pedro.lopes/data/audio_data/train/val/clean/mix/\" + item.rsplit('/')[-1]\n",
    "    !mv $item $dest\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev_pedro] *",
   "language": "python",
   "name": "conda-env-dev_pedro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
