{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import museval\n",
    "import musdb\n",
    "from utils import separate_from_audio,load_unet_spleeter\n",
    "import audiofile as af\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_and_evaluate(track):\n",
    "    # assume mix as estimates\n",
    "    estimates = {\n",
    "        'vocals': track,\n",
    "        'accompaniment': track\n",
    "    }\n",
    "\n",
    "    # Evaluate using museval\n",
    "    scores = museval.eval_mus_track(\n",
    "        track, estimates, output_dir=\"../results/\"\n",
    "    )\n",
    "\n",
    "    # print nicely formatted and aggregated scores\n",
    "    print(scores)\n",
    "def to_mono(audio):\n",
    "    return audio[0]+audio[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob(\"/nfs/home/pedro.lopes/data/dataset/musdb18hq/test/*\")\n",
    "mix_audios = []\n",
    "vocal_audios = []\n",
    "acc_audios = []\n",
    "for path in paths:\n",
    "    audio_mix, sr = af.read(path + '/mixture.wav' )\n",
    "    audio_vocal, sr = af.read(path + '/vocals.wav' )\n",
    "    audio_bass, sr = af.read(path + '/bass.wav' )\n",
    "    audio_drums, sr = af.read(path + '/drums.wav' )\n",
    "    audio_other, sr = af.read(path + '/other.wav' )\n",
    "    audio_mix = to_mono(audio_mix)\n",
    "    audio_vocal = to_mono(audio_vocal)\n",
    "    audio_acc = to_mono((audio_bass+audio_drums+audio_other)/3)\n",
    "    mix_audios.append(audio_mix)\n",
    "    vocal_audios.append(audio_vocal)\n",
    "    acc_audios.append(audio_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = '../checkpoints/best_4-6.hdf5'\n",
    "model = load_unet_spleeter((6,4),weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_acc = np.squeeze(audio_acc)\n",
    "audio_acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pred_vocals = []\n",
    "pred_accs = []\n",
    "for i in tqdm(range(len(mix_audios))):\n",
    "    audio_mix = mix_audios[i]\n",
    "    audio_vocal_pred = separate_from_audio(np.squeeze(audio_mix),44100,model)\n",
    "    audio_acc_pred = audio_mix[:len(audio_vocal_pred)] - audio_vocal_pred\n",
    "    pred_vocals.append(audio_vocal_pred)\n",
    "    pred_accs.append(audio_acc_pred)\n",
    "    \n",
    "#     audio_vocal = np.expand_dims(audio_vocal,axis=-1)\n",
    "#     audio_acc = np.expand_dims(audio_acc*3,axis=-1)\n",
    "#     audio_vocal_pred = np.expand_dims(audio_vocal_pred,axis=-1)\n",
    "#     audio_acc_pred = np.expand_dims(audio_acc_pred,axis=-1)\n",
    "#     reference_sources = np.array([audio_vocal,audio_acc])\n",
    "#     estimated_sources = np.array([audio_vocal_pred,audio_acc_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in tqdm(range(len(pred_vocals))):\n",
    "    estimated_sources = np.vstack([pred_vocals[i],pred_accs[i]])\n",
    "    estimated_sources = np.expand_dims(estimated_sources,axis=-1)\n",
    "    reference_sources = np.vstack([vocal_audios[i][:len(pred_vocals[i])],acc_audios[i][:len(pred_vocals[i])]*3])\n",
    "    reference_sources = np.expand_dims(reference_sources,axis=-1)\n",
    "    sdr,isr,sir,sar,perm=museval.metrics.bss_eval(reference_sources,estimated_sources,np.inf)\n",
    "    results.append([sdr,isr,sir,sar])\n",
    "    print([sdr,isr,sir,sar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(np.array(results),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(np.squeeze(audio_vocal_pred),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_len = 0\n",
    "for item in mix_audios:\n",
    "    full_len+=len(item)\n",
    "full_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_len/3600/44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(np.squeeze(audio_vocal),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdr,isr,sir,sar,perm=museval.metrics.bss_eval(reference_sources,estimated_sources,np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(range(len(paths))):\n",
    "    basename = paths[i].rsplit('/',1)[1]\n",
    "    base_path = '/nfs/home/pedro.lopes/data/results/u_net_4_6/'\n",
    "    af.write(base_path + basename + '_vocals.wav',pred_vocals[i], 44100)\n",
    "    af.write(base_path + basename + '_acc.wav',pred_accs[i], 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /nfs/home/pedro.lopes/data/results/u_net_4_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sdr(references, estimates):\n",
    "    # compute SDR for one song\n",
    "    delta = 1e-7  # avoid numerical errors\n",
    "    num = np.sum(np.square(references), axis=(1, 2))\n",
    "    den = np.sum(np.square(references - estimates), axis=(1, 2))\n",
    "    num += delta\n",
    "    den += delta\n",
    "    return 10 * np.log10(num  / den)\n",
    "calc_sdr(reference_sources,estimated_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(sdr[sdr>0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(sdr,axis=-1),np.median(isr,axis=-1),np.median(sir,axis=-1),np.median(sar,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openunmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openunmix import utils\n",
    "import openunmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(\n",
    "    audio,\n",
    "    rate=None,\n",
    "    model_str_or_path=\"umxhq\",\n",
    "    targets=None,\n",
    "    niter=1,\n",
    "    residual=False,\n",
    "    wiener_win_len=300,\n",
    "    aggregate_dict=None,\n",
    "    separator=None,\n",
    "    device=None,\n",
    "    filterbank=\"torch\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Open Unmix functional interface\n",
    "    Separates a torch.Tensor or the content of an audio file.\n",
    "    If a separator is provided, use it for inference. If not, create one\n",
    "    and use it afterwards.\n",
    "    Args:\n",
    "        audio: audio to process\n",
    "            torch Tensor: shape (channels, length), and\n",
    "            `rate` must also be provided.\n",
    "        rate: int or None: only used if audio is a Tensor. Otherwise,\n",
    "            inferred from the file.\n",
    "        model_str_or_path: the pretrained model to use\n",
    "        targets (str): select the targets for the source to be separated.\n",
    "            a list including: ['vocals', 'drums', 'bass', 'other'].\n",
    "            If you don't pick them all, you probably want to\n",
    "            activate the `residual=True` option.\n",
    "            Defaults to all available targets per model.\n",
    "        niter (int): the number of post-processingiterations, defaults to 1\n",
    "        residual (bool): if True, a \"garbage\" target is created\n",
    "        wiener_win_len (int): the number of frames to use when batching\n",
    "            the post-processing step\n",
    "        aggregate_dict (str): if provided, must be a string containing a '\n",
    "            'valid expression for a dictionary, with keys as output '\n",
    "            'target names, and values a list of targets that are used to '\n",
    "            'build it. For instance: \\'{\\\"vocals\\\":[\\\"vocals\\\"], '\n",
    "            '\\\"accompaniment\\\":[\\\"drums\\\",\\\"bass\\\",\\\"other\\\"]}\\'\n",
    "        separator: if provided, the model.Separator object that will be used\n",
    "             to perform separation\n",
    "        device (str): selects device to be used for inference\n",
    "        filterbank (str): filterbank implementation method.\n",
    "            Supported are `['torch', 'asteroid']`. `torch` is about 30% faster\n",
    "            compared to `asteroid` on large FFT sizes such as 4096. However,\n",
    "            asteroids stft can be exported to onnx, which makes is practical\n",
    "            for deployment.\n",
    "    \"\"\"\n",
    "    if separator is None:\n",
    "        separator = utils.load_separator(\n",
    "            model_str_or_path=model_str_or_path,\n",
    "            targets=targets,\n",
    "            niter=niter,\n",
    "            residual=residual,\n",
    "            wiener_win_len=wiener_win_len,\n",
    "            device=device,\n",
    "            pretrained=True,\n",
    "            filterbank=filterbank,\n",
    "        )\n",
    "        separator.freeze()\n",
    "        if device:\n",
    "            separator.to(device)\n",
    "\n",
    "    if rate is None:\n",
    "        raise Exception(\"rate` must be provided.\")\n",
    "\n",
    "    if device:\n",
    "        audio = audio.to(device)\n",
    "    audio = utils.preprocess(audio, rate, separator.sample_rate)\n",
    "\n",
    "    # getting the separated signals\n",
    "    estimates = separator(audio)\n",
    "    estimates = separator.to_dict(estimates, aggregate_dict=aggregate_dict)\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_mix = torch.Tensor(mix_audios[0])\n",
    "result = separate(audio_mix,rate=44100,model_str_or_path=\"../checkpoints/model6\",targets=['vocals'],residual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_vocal = np.array(result['vocals'][0][0])\n",
    "audio_acc = mix_audios[0] - audio_vocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_vocal,rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_acc,rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_vocal_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pred_vocals = []\n",
    "pred_accs = []\n",
    "for i in tqdm(range(len(mix_audios))):\n",
    "    audio_mix = torch.Tensor(mix_audios[i])\n",
    "    result = separate(audio_mix,rate=44100,model_str_or_path=\"../checkpoints/model6\",targets=['vocals'],residual=True)\n",
    "    audio_vocal_pred = np.array(result['vocals'][0][0])\n",
    "    audio_acc_pred = audio_mix[:len(audio_vocal_pred)] - audio_vocal_pred[:len(audio_mix)]\n",
    "    pred_vocals.append(audio_vocal_pred)\n",
    "    pred_accs.append(audio_acc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(paths))):\n",
    "    basename = paths[i].rsplit('/',1)[1]\n",
    "    base_path = '/nfs/home/pedro.lopes/data/results/blstm2/'\n",
    "    af.write(base_path + basename + '_vocals.wav',pred_vocals[i], 44100)\n",
    "    af.write(base_path + basename + '_acc.wav',pred_accs[i], 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /nfs/home/pedro.lopes/data/results/blstm2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in tqdm(range(len(pred_vocals))):\n",
    "    estimated_sources = np.vstack([pred_vocals[i][:len(pred_accs[i])],pred_accs[i]])\n",
    "    print(len(pred_vocals[i]),len(pred_accs[i]),len(vocal_audios[i]),len(acc_audios[i]))\n",
    "    estimated_sources = np.expand_dims(estimated_sources,axis=-1)\n",
    "    reference_sources = np.vstack([vocal_audios[i][:len(pred_vocals[i])],acc_audios[i][:len(pred_vocals[i])]*3])\n",
    "    reference_sources = np.expand_dims(reference_sources,axis=-1)\n",
    "    sdr,isr,sir,sar,perm=museval.metrics.bss_eval(reference_sources,estimated_sources,np.inf)\n",
    "    results.append([sdr,isr,sir,sar])\n",
    "    print([sdr,isr,sir,sar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(pred_vocals[1],rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmedian(np.array(results),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate=44100\n",
    "Audio(audio_mix[start_time*sample_rate:end_time*sample_rate],rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "basepaths=[]\n",
    "for path in paths:\n",
    "    basepaths.append(os.path.basename(path))\n",
    "basepaths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'u_net_5_5'\n",
    "rootdir ='/nfs/home/pedro.lopes/data/results/' + model + '/'\n",
    "number= 0\n",
    "audio_vocal = af.read(rootdir + basepaths[number] + '_vocals.wav')\n",
    "audio_acc = af.read(rootdir + basepaths[number] + '_acc.wav')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev_pedro] *",
   "language": "python",
   "name": "conda-env-dev_pedro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
