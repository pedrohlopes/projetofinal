{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import datetime\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import tensorflow as tf\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from sklearn import preprocessing\n",
    "from mpi4py import MPI\n",
    "import json\n",
    "from librosa.core import resample\n",
    "import os\n",
    "import audiofile as af\n",
    "from functools import partial\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5bQx4qlAISj"
   },
   "outputs": [],
   "source": [
    "def normalize(x,save=False):\n",
    "    scaled_x = (x - np.mean(x))/(np.abs(np.std(x))+1e-8)\n",
    "\n",
    "    if save:\n",
    "        return scaled_x, np.mean(x), np.std(x)\n",
    "    return scaled_x\n",
    "def normalize_from_outer(x,x_mean,x_std):\n",
    "    scaled_x = (x - x_mean)/(x_std+1e-8)\n",
    "    return scaled_x\n",
    "def preprocess(sample,x_mean,x_std):\n",
    "    log_sample = np.log(np.abs(sample)+1e-7)\n",
    "    mod_input = normalize_from_outer(log_sample,x_mean,x_std)\n",
    "    return mod_input\n",
    "def denormalize(x,x_mean,x_std):\n",
    "    scaled_x = x*(x_std + 1e-8) + x_mean\n",
    "    return scaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import musdb\n",
    "# mus = musdb.DB('/nfs/home/pedro.lopes/data/dataset/musdb18hq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "json_path = '../norm_data_full.json'\n",
    "with open(json_path) as infile:\n",
    "        norm_data = json.load(infile)\n",
    "dataset_path = '/home/pedro.lopes/data/audio_data/train/augmented/'\n",
    "X_mean = norm_data['X_min']\n",
    "X_std = norm_data['X_max'] - norm_data['X_min']\n",
    "\n",
    "def preprocess_audio(audio,rate,return_stft=False):\n",
    "    samples = []\n",
    "    num_samples = 20\n",
    "    offset = 1500\n",
    "    freq_bins = 512\n",
    "    sample_len = 128\n",
    "    stft_data = tf.signal.stft(audio, \n",
    "\tframe_length=1024, \n",
    "\tframe_step=512,\n",
    "\tfft_length=1024).numpy().T\n",
    "    \n",
    "\n",
    "\n",
    "    X_mean = norm_data['X_min']\n",
    "    X_std = norm_data['X_max'] - norm_data['X_min']\n",
    "    mod_input = np.zeros((int(sample_len*np.ceil(len(stft_data[0])/sample_len)),513),dtype = complex)\n",
    "    mod_input[0:len(stft_data[0])]= np.array(stft_data.T)\n",
    "\n",
    "    \n",
    "    range_check = range(0,int(sample_len*np.ceil(len(stft_data[0])/sample_len)),sample_len)\n",
    "    for i in range_check:\n",
    "        samples.append(mod_input[i:i+sample_len,1:513].T.reshape(freq_bins,sample_len,1))\n",
    "    mod_input_array = preprocess(np.array(samples),X_mean,X_std)\n",
    "    if return_stft:\n",
    "        return mod_input_array,stft_data,samples\n",
    "    return mod_input_array\n",
    "\n",
    "def normalize(x,save=False):\n",
    "    scaled_x = (x - np.mean(x))/(np.abs(np.std(x))+1e-8)\n",
    "\n",
    "    if save:\n",
    "      return scaled_x, np.mean(x), np.std(x)\n",
    "    return scaled_x\n",
    "def normalize_from_outer(x,x_mean,x_std):\n",
    "  scaled_x = (x - x_mean)/(x_std+1e-8)\n",
    "  return scaled_x\n",
    "def preprocess(sample,x_mean,x_std):\n",
    "  log_sample = np.log(np.abs(sample)+1e-7)\n",
    "  mod_input = normalize_from_outer(log_sample,x_mean,x_std)\n",
    "  return mod_input\n",
    "\n",
    "def preprocess_tf(sample,x_mean,x_std):\n",
    "  log_sample = tf.math.log(tf.math.abs(sample)+1e-7)\n",
    "  mod_input = normalize_from_outer(log_sample,x_mean,x_std)\n",
    "  return mod_input\n",
    "\n",
    "def denormalize(x,x_mean,x_std):\n",
    "  scaled_x = x*(x_std + 1e-8) + x_mean\n",
    "  return scaled_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LENGHT = 4096\n",
    "HOP_SIZE =1024\n",
    "N_BINS = 2048\n",
    "# def normalize(x,save=False):\n",
    "#     scaled_x = (x - np.mean(x))/(np.abs(np.std(x))+1e-8)\n",
    "\n",
    "#     if save:\n",
    "#       return scaled_x, np.mean(x), np.std(x)\n",
    "#     return scaled_x\n",
    "# def normalize_from_outer(x,x_mean,x_std):\n",
    "#   scaled_x = (x - x_mean)/(x_std+1e-8)\n",
    "#   return scaled_x\n",
    "# def preprocess(sample,x_mean,x_std):\n",
    "#   log_sample = np.log(np.abs(sample)+1e-7)\n",
    "#   mod_input = normalize_from_outer(log_sample,x_mean,x_std)\n",
    "#   return mod_input\n",
    "\n",
    "# def preprocess_tf(sample,x_mean,x_std):\n",
    "#   log_sample = tf.math.log(tf.math.abs(sample)+1e-7)\n",
    "#   mod_input = normalize_from_outer(log_sample,x_mean,x_std)\n",
    "#   return mod_input\n",
    "\n",
    "# def denormalize(x,x_mean,x_std):\n",
    "#   scaled_x = x*(x_std + 1e-8) + x_mean\n",
    "#   return scaled_x\n",
    "\n",
    "\n",
    "# def pad_tf(stft_data):\n",
    "#     x_before = tf.transpose(stft_data)\n",
    "#     x_shape = tf.cast(tf.shape(x_before)[-1],tf.float64)\n",
    "#     print(x_shape)\n",
    "#     pad_len = tf.math.floor(tf.math.ceil(x_shape/128)*128 - x_shape)\n",
    "#     pad = ([0,0],[0,pad_len])\n",
    "#     x = tf.pad(x_before,pad,mode='constant', constant_values=0)\n",
    "#     x_split = tf.split(x,4,axis = -1)\n",
    "#     return x_split\n",
    "\n",
    "# def preprocess_audio_tf(x,y):\n",
    "#     with tf.device('GPU:1'):\n",
    "#         stft_data = tf.signal.stft(x, \n",
    "#         frame_length=FRAME_LENGHT, \n",
    "#         frame_step=HOP_SIZE,\n",
    "#         fft_length=FRAME_LENGHT)\n",
    "#         x = pad_tf(stft_data)\n",
    "#         x = preprocess_tf(x,X_mean,X_std)[:,0:N_BINS]\n",
    "#         x = tf.expand_dims(x,axis=-1)\n",
    "#         stft_data = tf.signal.stft(y, \n",
    "#         frame_length=FRAME_LENGHT, \n",
    "#         frame_step=HOP_SIZE,\n",
    "#         fft_length=FRAME_LENGHT)\n",
    "#         y = pad_tf(stft_data)\n",
    "#         y = preprocess_tf(y,X_mean,X_std)[:,0:N_BINS]\n",
    "#         y = tf.expand_dims(y,axis=-1)\n",
    "#     return x,y\n",
    "# def pad_tf_test(stft_data,test=False):\n",
    "#     x_before = tf.keras.backend.permute_dimensions(stft_data,(0,2,1))\n",
    "#     x_shape = tf.cast(tf.shape(x_before)[-1],tf.float64)\n",
    "#     pad_len = tf.math.floor(tf.math.ceil(x_shape/128)*128 - x_shape)\n",
    "#     pad = ([0,0],[0,0],[0,pad_len])\n",
    "#     x_padded = tf.pad(x_before,pad,mode='constant', constant_values=0)\n",
    "#     if test:\n",
    "#         x_split = tf.split(x_padded,int(x_padded.shape[-1]/128),axis = -1)\n",
    "#     else:\n",
    "#         x_split = tf.split(x_padded,4,axis = -1)\n",
    "#     x = tf.concat(x_split,axis=0)\n",
    "#     return x,x_padded\n",
    "\n",
    "\n",
    "# def preprocess_audio_tf_test(l_input,test=False):\n",
    "#     stft_data = tf.signal.stft(l_input, \n",
    "#       frame_length=FRAME_LENGHT, \n",
    "#       frame_step=HOP_SIZE,\n",
    "#       fft_length=FRAME_LENGHT)\n",
    "#     x,x_padded = pad_tf_test(stft_data,test)\n",
    "#     x_padded = tf.squeeze(x_padded)\n",
    "#     x = preprocess_tf(x,X_mean,X_std)[:,0:N_BINS]\n",
    "#     x = tf.expand_dims(x,axis=-1)\n",
    "#     if test==True:\n",
    "#         return x,x_padded\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXRD0-Wj5Prs"
   },
   "outputs": [],
   "source": [
    "# def split_stack_audio(audio):\n",
    "#     audio_pad = np.pad(audio,(0,441000 - audio.shape[-1]%441000), 'constant', constant_values=0)\n",
    "#     audio_input = np.stack(np.split(audio_pad,441000)).T\n",
    "#     return audio_input\n",
    "\n",
    "\n",
    "# def preprocess_audio(audio,rate):\n",
    "#     samples = []\n",
    "#     num_samples = 20\n",
    "#     offset = 1500\n",
    "#     freq_bins = 512\n",
    "#     sample_len = 128\n",
    "    \n",
    "#     audio_split = split_stack_audio(audio)\n",
    "#     audio_stft_list = []\n",
    "#     for i in range(len(audio_split)):\n",
    "#         stft_data = tf.signal.stft(audio_split[i], \n",
    "#         frame_length=1024, \n",
    "#         frame_step=512,\n",
    "#         fft_length=1024).numpy().T\n",
    "#         stft_list.append(audio)\n",
    "#     stft_array = np.stack(stft_list)\n",
    "#     json_path = '../norm_data_full.json'\n",
    "#     with open(json_path) as infile:\n",
    "#         norm_data = json.load(infile)\n",
    "#     X_mean = norm_data['X_min']\n",
    "#     X_std = norm_data['X_max'] - norm_data['X_min']\n",
    "#     mod_input = np.zeros((int(sample_len*np.ceil(len(stft_data[0])/sample_len)),513),dtype = complex)\n",
    "#     mod_input[0:len(stft_data[0])]= np.array(stft_data.T)\n",
    "    \n",
    "    \n",
    "#     range_check = range(0,int(sample_len*np.ceil(len(stft_data[0])/sample_len)),sample_len)\n",
    "#     for i in range_check:\n",
    "#         samples.append(mod_input[i:i+sample_len,1:513].T.reshape(freq_bins,sample_len,1))\n",
    "#     mod_input_array = preprocess(np.array(samples),X_mean,X_std)\n",
    "#     return mod_input_array,stft_data,samples\n",
    "\n",
    "def pad_tf(stft_data,test=False):\n",
    "    x_before = tf.keras.backend.permute_dimensions(stft_data,(0,2,1))\n",
    "    x_shape = tf.cast(tf.shape(x_before)[-1],tf.float64)\n",
    "    pad_len = tf.math.floor(tf.math.ceil(x_shape/128)*128 - x_shape)\n",
    "    pad = ([0,0],[0,0],[0,pad_len])\n",
    "    x_padded = tf.pad(x_before,pad,mode='constant', constant_values=0)\n",
    "    if test:\n",
    "        x_split = tf.split(x_padded,int(x_padded.shape[-1]/128),axis = -1)\n",
    "    else:\n",
    "        x_split = tf.split(x_padded,7,axis = -1)\n",
    "    x = tf.concat(x_split,axis=0)\n",
    "    return x,x_padded\n",
    "\n",
    "def preprocess_audio_tf(l_input,test=False):\n",
    "    stft_data = tf.signal.stft(l_input, \n",
    "    frame_length=FRAME_LENGHT, \n",
    "    frame_step=HOP_SIZE,\n",
    "    fft_length=FRAME_LENGHT)\n",
    "    x,x_padded = pad_tf(stft_data,test)\n",
    "    x_padded = tf.squeeze(x_padded)\n",
    "    x = preprocess_tf(x,X_mean,X_std)[:,0:N_BINS]\n",
    "    x = tf.expand_dims(x,axis=-1)\n",
    "    if test==True:\n",
    "        return x,x_padded\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_umx(input_spec,freq_bins,sample_len):\n",
    "    DROPOUT = 0.4\n",
    "    hidden_size = 512\n",
    "    print(input_spec.shape)\n",
    "    l1 = K.permute_dimensions(input_spec,(0,2,1,3))\n",
    "    l1 = K.reshape(l1,(-1,freq_bins))\n",
    "    #l1 = input_spec\n",
    "    l2 = Dense(hidden_size)(l1)\n",
    "    l2 = BatchNormalization()(l2)\n",
    "    l2 = K.reshape(l2,(128,-1,hidden_size))\n",
    "\n",
    "\n",
    "    l3 = Activation('tanh')(l2)\n",
    "    l4 = K.reshape(l3,(sample_len,-1,hidden_size))\n",
    "    #l4 = K.expand_dims(l4,axis=-1)\n",
    "    x1 = Bidirectional(LSTM(int(hidden_size/2), return_sequences=True,dropout=DROPOUT))(l4)\n",
    "\n",
    "    #x1 = Dropout(0.4)(x1)\n",
    "\n",
    "    x2 = Bidirectional(LSTM(int(hidden_size/2), return_sequences=True,dropout=DROPOUT))(x1)\n",
    "    #x2 = Dropout(0.4)(x2)\n",
    "    \n",
    "    x3 = Bidirectional(LSTM(int(hidden_size/2), return_sequences=True,dropout=DROPOUT))(x2)\n",
    "    #x3 = Dropout(0.4)(x3)\n",
    "    \n",
    "    \n",
    "    #x3 = tf.keras.layers.TimeDistributed(Dense(freq_bins))(x3)\n",
    "    \n",
    "    \n",
    "    c1 = concatenate([l4,x3],axis=2)\n",
    "    c1 = tf.reshape(c1,(-1,c1.shape[-1]))\n",
    "    c2 = Dense(hidden_size)(c1)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c3 = Activation('relu')(c2)\n",
    "    c4 = Dense(freq_bins)(c3)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c5 = Activation('relu')(c4)\n",
    "    #c6 = K.permute_dimensions(c5,(1,0))\n",
    "    c6 = K.reshape(c5,(-1,sample_len,freq_bins))\n",
    "    c6 = K.permute_dimensions(c6,(0,2,1))\n",
    "    c6 = K.reshape(c6,(-1,freq_bins,sample_len,1))\n",
    "    return c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_spleeter(input_tensor, kernel_size=(5, 5), strides=(2, 2)):\n",
    "    DROPOUT = 0\n",
    "    conv_activation_layer = LeakyReLU(0.2)\n",
    "    deconv_activation_layer = ReLU()\n",
    "    conv_n_filters = [16, 32, 64, 128, 256, 512]\n",
    "    kernel_initializer = 'he_normal'\n",
    "    conv2d_factory = partial(\n",
    "        Conv2D, strides=strides, padding=\"same\", kernel_initializer=kernel_initializer\n",
    "    )\n",
    "    # First layer.\n",
    "    conv1 = conv2d_factory(conv_n_filters[0], kernel_size)(input_tensor)\n",
    "    batch1 = BatchNormalization(axis=-1)(conv1)\n",
    "    rel1 = conv_activation_layer(batch1)\n",
    "    # Second layer.\n",
    "    conv2 = conv2d_factory(conv_n_filters[1], kernel_size)(rel1)\n",
    "    batch2 = BatchNormalization(axis=-1)(conv2)\n",
    "    rel2 = conv_activation_layer(batch2)\n",
    "    # Third layer.\n",
    "    conv3 = conv2d_factory(conv_n_filters[2], kernel_size)(rel2)\n",
    "    batch3 = BatchNormalization(axis=-1)(conv3)\n",
    "    rel3 = conv_activation_layer(batch3)\n",
    "    # Fourth layer.\n",
    "    conv4 = conv2d_factory(conv_n_filters[3], kernel_size)(rel3)\n",
    "    batch4 = BatchNormalization(axis=-1)(conv4)\n",
    "    rel4 = conv_activation_layer(batch4)\n",
    "    # Fifth layer.\n",
    "    conv5 = conv2d_factory(conv_n_filters[4], kernel_size)(rel4)\n",
    "    batch5 = BatchNormalization(axis=-1)(conv5)\n",
    "    rel5 = conv_activation_layer(batch5)\n",
    "    # Sixth layer\n",
    "    conv6 = conv2d_factory(conv_n_filters[5], kernel_size)(rel5)\n",
    "    batch6 = BatchNormalization(axis=-1)(conv6)\n",
    "    _ = conv_activation_layer(batch6)\n",
    "    #\n",
    "    #\n",
    "    conv2d_transpose_factory = partial(\n",
    "        Conv2DTranspose,\n",
    "        strides=strides,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )\n",
    "    #\n",
    "    up1 = conv2d_transpose_factory(conv_n_filters[4], kernel_size)((conv6))\n",
    "    up1 = deconv_activation_layer(up1)\n",
    "    batch7 = BatchNormalization(axis=-1)(up1)\n",
    "    drop1 = Dropout(DROPOUT)(batch7)\n",
    "    merge1 = Concatenate(axis=-1)([conv5, drop1])\n",
    "    #\n",
    "    up2 = conv2d_transpose_factory(conv_n_filters[3], kernel_size)((merge1))\n",
    "    up2 = deconv_activation_layer(up2)\n",
    "    batch8 = BatchNormalization(axis=-1)(up2)\n",
    "    drop2 = Dropout(DROPOUT)(batch8)\n",
    "    merge2 = Concatenate(axis=-1)([conv4, drop2])\n",
    "    #\n",
    "    up3 = conv2d_transpose_factory(conv_n_filters[2], kernel_size)((merge2))\n",
    "    up3 = deconv_activation_layer(up3)\n",
    "    batch9 = BatchNormalization(axis=-1)(up3)\n",
    "    drop3 = Dropout(DROPOUT)(batch9)\n",
    "    merge3 = Concatenate(axis=-1)([conv3, drop3])\n",
    "    #\n",
    "    up4 = conv2d_transpose_factory(conv_n_filters[1], kernel_size)((merge3))\n",
    "    up4 = deconv_activation_layer(up4)\n",
    "    batch10 = BatchNormalization(axis=-1)(up4)\n",
    "    merge4 = Concatenate(axis=-1)([conv2, batch10])\n",
    "    #\n",
    "    up5 = conv2d_transpose_factory(conv_n_filters[0], kernel_size)((merge4))\n",
    "    up5 = deconv_activation_layer(up5)\n",
    "    batch11 = BatchNormalization(axis=-1)(up5)\n",
    "    merge5 = Concatenate(axis=-1)([conv1, batch11])\n",
    "    #\n",
    "    up6 = conv2d_transpose_factory(1, kernel_size)((merge5))\n",
    "    up6 = deconv_activation_layer(up6)\n",
    "    batch12 = BatchNormalization(axis=-1)(up6)\n",
    "    # Last layer to ensure initial shape reconstruction.\n",
    "\n",
    "    up7 = Conv2D(\n",
    "        1,\n",
    "        (4, 4),\n",
    "        dilation_rate=(2, 2),\n",
    "        activation=\"sigmoid\",\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )((batch12))\n",
    "    return up7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = (5,5),stride = 1, batchnorm = False):\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = kernel_size,\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same', strides=(stride, stride))(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(tf.nn.leaky_relu)(x)\n",
    "    \n",
    "#     # second layer\n",
    "#     x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "#               kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "#     if batchnorm:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Activation(tf.nn.leaky_relu)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def conv2d_transpose_block(input_tensor,n_filters,kernel_size = (5,5),strides = (2,2), batchnorm = False):\n",
    "    x = Conv2DTranspose(n_filters, kernel_size = kernel_size, strides = strides, padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(tf.nn.relu)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def get_unet_mask(input_img, n_filters=16, dropout=0.5, kernel_size=(5, 5), batchnorm=True):\n",
    "\n",
    "    # Contracting Path 1\n",
    "    c1 = conv2d_block(input_img, n_filters * 1,\n",
    "                      kernel_size=kernel_size, batchnorm=batchnorm, stride=2)\n",
    "    #c1 = MaxPooling2D((2, 2))(c1)\n",
    "    #c1 = Dropout(dropout)(c1)\n",
    "\n",
    "    c2 = conv2d_block(c1, n_filters * 2, kernel_size=kernel_size,\n",
    "                      batchnorm=batchnorm, stride=2)\n",
    "    #c2 = MaxPooling2D((2, 2))(c2)\n",
    "    #c2 = Dropout(dropout)(c2)\n",
    "\n",
    "    c3 = conv2d_block(c2, n_filters * 4, kernel_size=kernel_size,\n",
    "                      batchnorm=batchnorm, stride=2)\n",
    "    #c3 = MaxPooling2D((2, 2))(c3)\n",
    "    #c3 = Dropout(dropout)(c3)\n",
    "\n",
    "    c4 = conv2d_block(c3, n_filters * 8, kernel_size=kernel_size,\n",
    "                      batchnorm=batchnorm, stride=2)\n",
    "    #c4 = MaxPooling2D((2, 2))(c4)\n",
    "    #c4 = Dropout(dropout)(c4)\n",
    "\n",
    "    c5 = conv2d_block(c4, n_filters=n_filters * 16,\n",
    "                      kernel_size=kernel_size, batchnorm=batchnorm, stride=2)\n",
    "    #c5= MaxPooling2D((2, 2))(c5)\n",
    "    #c5 = Dropout(dropout)(c5)\n",
    "\n",
    "    c6 = conv2d_block(c5, n_filters=n_filters * 32,\n",
    "                      kernel_size=kernel_size, batchnorm=batchnorm, stride=2)\n",
    "\n",
    "    # Expansive Path 1\n",
    "    u3 = conv2d_transpose_block(c6, n_filters=n_filters * 16,\n",
    "                                kernel_size=kernel_size, strides=(2, 2), batchnorm=batchnorm)\n",
    "    u5 = concatenate([u3, c5])\n",
    "    u5 = Dropout(dropout)(u5)\n",
    "\n",
    "    u61 = conv2d_transpose_block(\n",
    "        u5, n_filters=n_filters * 8, kernel_size=kernel_size, strides=(2, 2), batchnorm=batchnorm)\n",
    "    u6 = concatenate([u61, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    #c6 = conv2d_block(u6, n_filters * 8, kernel_size = 5, batchnorm = batchnorm)\n",
    "\n",
    "    u71 = conv2d_transpose_block(\n",
    "        u6, n_filters=n_filters * 4, kernel_size=kernel_size, strides=(2, 2), batchnorm=batchnorm)\n",
    "    u7 = concatenate([u71, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    #c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u81 = conv2d_transpose_block(\n",
    "        u7, n_filters=n_filters * 2, kernel_size=kernel_size, strides=(2, 2), batchnorm=batchnorm)\n",
    "    u8 = concatenate([u81, c2])\n",
    "    #u8 = Dropout(dropout)(u8)\n",
    "    #c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u91 = conv2d_transpose_block(\n",
    "        u8, n_filters=n_filters * 1, kernel_size=kernel_size, strides=(2, 2), batchnorm=batchnorm)\n",
    "    u9 = concatenate([u91, c1])\n",
    "    #u9 = Dropout(dropout)(u9)\n",
    "    #c9 = conv2d_block(u9, n_filters * 1, kernel_size = 5, batchnorm = batchnorm, stride=2)\n",
    "    c9 = Conv2DTranspose(1, kernel_size=kernel_size, strides=(\n",
    "        2, 2), padding='same', activation='sigmoid')(u9)\n",
    "    #mask_layer = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    #l_out = Multiply()([l_input,mask_layer])\n",
    "    #model = Model(inputs=[input_img], outputs=[l_out,l_out_2])\n",
    "    return c9\n",
    "    #return mask_layer, l_out\n",
    "with tf.device('GPU:1'):\n",
    "    init = keras.initializers.glorot_normal(seed=None)\n",
    "    reg = 5e-5\n",
    "    regularizer = tf.keras.regularizers.l2(reg)\n",
    "    freq_bins = 2048\n",
    "    sample_len = 128\n",
    "    l_input = Input(shape=(freq_bins, 128, 1))\n",
    "#     l_out_1 = get_unet_spleeter(l_input, kernel_size=(7, 3))\n",
    "#     l_out_2 = get_unet_spleeter(l_input, kernel_size=(3, 7))\n",
    "#     concat_layer = concatenate([l_out_1, l_out_2])\n",
    "#     mask_layer = Conv2D(1, (1, 1))(concat_layer)\n",
    "#     final_layer = Multiply()([l_input, mask_layer])\n",
    "#     mask_layer= get_unet_spleeter(l_input, kernel_size=(5, 5))\n",
    "#     final_layer = Multiply()([l_input, mask_layer])\n",
    "    mask_layer= get_umx(l_input, 2048,128)\n",
    "    final_layer = Multiply()([l_input, mask_layer])\n",
    "#     l_input = Input(shape=(None,))\n",
    "#     final_layer,mask_layer = get_unet_mask(l_input,kernel_size = (5,5))\n",
    "\n",
    "#     l_out_2= get_unet_mask(l_input,kernel_size = (2,5))\n",
    "\n",
    "    #l_out_1_2= get_unet_mask(l_input,kernel_size = (5,2))\n",
    "    #l_out_2_2= get_unet_mask(l_input,kernel_size = (2,5))\n",
    "\n",
    "    #concat_layer_2 = concatenate([l_out_1_2,l_out_2_2])\n",
    "    #mask_layer_2 = Conv2D(1, (1, 1), activation='sigmoid')(concat_layer_2)\n",
    "    #final_layer_2 = Multiply()([l_input,mask_layer_2])\n",
    "\n",
    "    model = Model(inputs=[l_input], outputs=[final_layer])\n",
    "    #model.load_weights('../checkpoints/umx_globo.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LENGHT = 4096\n",
    "HOP_SIZE =1024\n",
    "N_BINS = 2048\n",
    "def separate_from_audio(audio,rate,mask_model,wiener_filter=True, return_spectrogram = False):\n",
    "    split_stft, full_stft = preprocess_audio_tf(np.expand_dims(audio,axis=0),test=True)\n",
    "    mask = mask_model.predict(split_stft)\n",
    "    #objective = preprocess(np.array(np.hstack(objective_vocal_samples)),X_mean,X_std)\n",
    "    mask_in_shape = np.concatenate(mask,axis=1)[:,:,0]\n",
    "    input_in_shape = full_stft\n",
    "    json_path = '../norm_data_full.json'\n",
    "    with open(json_path) as infile:\n",
    "        norm_data = json.load(infile)\n",
    "    X_mean = norm_data['X_min']\n",
    "    X_std = norm_data['X_max'] - norm_data['X_min']\n",
    "    test_sample = np.zeros((N_BINS + 1,input_in_shape.shape[1]), dtype=complex)\n",
    "    test_sample[0:N_BINS + 1] = full_stft[0:N_BINS + 1]\n",
    "    mask_final = np.zeros((N_BINS + 1,test_sample.shape[1]))\n",
    "    final_mag = np.zeros((N_BINS + 1,test_sample.shape[1]))\n",
    "\n",
    "    mask_final[0:N_BINS] = np.concatenate(mask,axis=1)[:,:,0]\n",
    "    pre_result= preprocess(test_sample,X_mean,X_std)\n",
    "\n",
    "\n",
    "    final_mag = denormalize(mask_final,X_mean,X_std)\n",
    "    result_stft = np.multiply(np.exp(final_mag),np.exp(1j*np.angle(test_sample)))\n",
    "\n",
    "    audio_vocal_pred = tf.signal.inverse_stft(result_stft.T,frame_length=FRAME_LENGHT, \n",
    "    frame_step=HOP_SIZE,\n",
    "    fft_length=FRAME_LENGHT,\n",
    "    window_fn=tf.signal.inverse_stft_window_fn(N_BINS)).numpy()\n",
    "    if wiener_filter:\n",
    "        test_sample_T = test_sample.T[:,:,np.newaxis]\n",
    "        result_stft_T = result_stft.T[:,:,np.newaxis,np.newaxis]\n",
    "\n",
    "\n",
    "        v = norbert.contrib.residual_model(np.abs(result_stft_T),test_sample_T)\n",
    "        result_wiener = norbert.wiener(v,test_sample_T,iterations=2)[:,:,:,0]\n",
    "        result_stft = result_wiener.T.reshape(final_mag.shape[0],final_mag.shape[1])\n",
    "        audio_vocal_pred = tf.signal.inverse_stft(result_stft.T,frame_length=FRAME_LENGHT, \n",
    "        frame_step=HOP_SIZE,\n",
    "        fft_length=FRAME_LENGHT,\n",
    "        window_fn=tf.signal.inverse_stft_window_fn(N_BINS)).numpy()\n",
    "\n",
    "    \n",
    "    if return_spectrogram:\n",
    "        return result_stft,mask_in_shape,preprocess_tf(input_in_shape,X_mean,X_std)\n",
    "    return audio_vocal_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "number=41\n",
    "filenames_mix = glob('/nfs/home/pedro.lopes/data/audio_data/train/val/clean/mix/*')\n",
    "filenames_vocal = glob('/nfs/home/pedro.lopes/data/audio_data/train/val/clean/vocals/*')\n",
    "audio,sr = af.read(filenames_mix[number])\n",
    "#audio,sr = af.read('/nfs/home/pedro.lopes/data/audio_data/train/augmented/val/sample_1_mix.wav')\n",
    "stft_data = tf.signal.stft(audio, \n",
    "    frame_length=FRAME_LENGHT, \n",
    "    frame_step=HOP_SIZE,\n",
    "    fft_length=FRAME_LENGHT)\n",
    "\n",
    "json_path = '../norm_data_full.json'\n",
    "with open(json_path) as infile:\n",
    "    norm_data = json.load(infile)\n",
    "X_mean = norm_data['X_min']\n",
    "X_std = norm_data['X_max'] - norm_data['X_min']\n",
    "audio_vocal,sr = af.read(filenames_vocal[number])\n",
    "#audio_vocal, sr = af.read('/nfs/home/pedro.lopes/data/audio_data/train/val/clean/sample_1_vocals.wav')\n",
    "stft_data = tf.signal.stft(audio_vocal, \n",
    "    frame_length=FRAME_LENGHT, \n",
    "    frame_step=HOP_SIZE,\n",
    "    fft_length=FRAME_LENGHT)\n",
    "objective = np.squeeze(preprocess_tf(stft_data,X_mean,X_std)).T\n",
    "#audio +=audio_vocal\n",
    "import norbert\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (24.0, 10.0)\n",
    "fig, axes = plt.subplots(ncols=2,nrows=2)\n",
    "#axes[1,0].pcolormesh(np.abs(test[0,:,:,0]))\n",
    "sample_rate=44100\n",
    "with tf.device('GPU:1'):\n",
    "    mask_model = Model(l_input, mask_layer)\n",
    "\n",
    "    result_stft,mask_in_shape,input_in_shape = separate_from_audio(audio,sample_rate,model,wiener_filter=True,return_spectrogram=True)\n",
    "axes[1,0].pcolormesh(np.log(np.abs(result_stft)+1e-7))\n",
    "axes[0,1].pcolormesh(mask_in_shape)\n",
    "axes[1,1].pcolormesh(objective)\n",
    "axes[0,0].pcolormesh(input_in_shape)\n",
    "\n",
    "axes[1,0].set_title('Input mascarado')\n",
    "axes[0,1].set_title('MÃ¡scara')\n",
    "axes[1,1].set_title('Ground Truth')\n",
    "axes[0,0].set_title('Input')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio = data\n",
    "audio_vocal_pred = separate_from_audio(audio,sample_rate,model,wiener_filter=True)\n",
    "ipd.Audio(audio_vocal_pred,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_acc = audio - audio_vocal_pred[0:len(audio)]\n",
    "ipd.Audio(audio_acc,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "table=pd.DataFrame(columns=[\"Name\",\"Layer type\",\"Output shape\", \"Number of parameters\"])\n",
    "for layer in model.layers:\n",
    "    try:\n",
    "        table = table.append({\"Name\":layer.name, \"Layer type\": layer.__class__.__name__,\"Output shape\":layer.output_shape, \"Number of parameters\": layer.count_params()}, ignore_index=True)\n",
    "    except: \n",
    "        pass\n",
    "table#.to_csv('original_unet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signal = tf.ones_like(audio)\n",
    "stft_data = tf.signal.stft(test_signal, \n",
    "    frame_length=1024, \n",
    "    frame_step=512,\n",
    "    fft_length=1024)\n",
    "output = tf.signal.inverse_stft(stft_data, \n",
    "    frame_length=1024, \n",
    "    frame_step=512,\n",
    "    window_fn=tf.signal.inverse_stft_window_fn(512))\n",
    "plt.plot(output[50000:60000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "alpha = 1\n",
    "audio_acc_pred = audio - alpha*audio_vocal_pred[0:len(audio)]\n",
    "reference_sources = np.asarray([audio_vocal,audio_acc_orig])\n",
    "estimated_sources = np.asarray([audio_vocal_pred[0:len(audio)],audio_acc_pred])\n",
    "_,sir_old,_,_ = mir_eval.separation.bss_eval_sources(reference_sources, estimated_sources)\n",
    "num_iter = 100\n",
    "step = 0.01\n",
    "derivative = 0.3\n",
    "tol=1e-4\n",
    "for i in range(num_iter):\n",
    "    alpha_new = alpha + derivative*step\n",
    "    #alpha_new = alpha - step\n",
    "    audio_acc_pred = audio - alpha_new*audio_vocal_pred[0:len(audio)]\n",
    "    reference_sources = np.asarray([audio_vocal_orig,audio_acc_orig])\n",
    "    estimated_sources = np.asarray([audio_vocal_pred[0:len(audio)],audio_acc_pred])\n",
    "    _,sir_new,_,_ = mir_eval.separation.bss_eval_sources(reference_sources, estimated_sources)\n",
    "    delta = (sir_new[1] - sir_old[1])\n",
    "    derivative = delta/(alpha_new-alpha)\n",
    "    print(derivative,alpha_new,sir_new[1])\n",
    "    alpha = alpha_new\n",
    "    if abs(delta)<= tol:\n",
    "        print('achei!')\n",
    "        break\n",
    "    sir_old = sir_new\n",
    "    \n",
    "print('terminei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"/home/pedro.lopes/data/audio_data/train/augmented/train/*/*/*\"\n",
    "names = glob(pattern)\n",
    "names_corrected =[]\n",
    "for name in names:\n",
    "    splitted = name.rsplit('_',maxsplit=1)[0]\n",
    "    names_corrected.append(splitted)\n",
    "\n",
    "paths = list(dict.fromkeys(names_corrected))\n",
    "paths_tuples = [(path + '_mix.wav', path + '_vocals.wav') for path in paths]\n",
    "\n",
    "number = 10\n",
    "paths_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_acc_pred = audio - alpha_new*audio_vocal_pred[0:len(audio)]\n",
    "ipd.Audio(audio_acc_pred,rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NSDR(sdr,audio_mixed,audio_vocal):\n",
    "    reference_sources = np.asarray([audio_vocal])\n",
    "    estimated_sources = np.asarray([audio_mixed])\n",
    "\n",
    "    sdr_base, _,_,_ = mir_eval.separation.bss_eval_sources(reference_sources, estimated_sources)\n",
    "    return sdr - sdr_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "range_sdr = range(100,150,1)\n",
    "sdr = np.zeros((50,2))\n",
    "sir = np.zeros((50,2))\n",
    "sar = np.zeros((50,2))\n",
    "nsdr = np.zeros((50,2))\n",
    "for i in tqdm(range_sdr):\n",
    "    audio = mus[i].audio[:,0] + mus[i].audio[:,1]\n",
    "    vocal_target = mus[i].targets['vocals'].audio[:,0] + mus[i].targets['vocals'].audio[:,1]\n",
    "    acc_target = mus[i].targets['accompaniment'].audio[:,0] + mus[i].targets['accompaniment'].audio[:,1]\n",
    "    reference_sources = np.asarray([vocal_target,acc_target])\n",
    "    audio_vocal_pred = separate_from_audio(audio,rate,mask_model)[0:len(audio)]\n",
    "    print(len(audio_vocal_pred))\n",
    "    audio_acc_pred = audio - audio_vocal_pred[0:len(audio)]\n",
    "    estimated_sources = np.asarray([audio_vocal_pred,audio_acc_pred])\n",
    "    (sdr[i-100,:], sir[i-100,:], sar[i-100,:], perm) = mir_eval.separation.bss_eval_sources(reference_sources, estimated_sources)\n",
    "    nsdr[i-100,0] = get_NSDR(sdr[i-100,0],audio,vocal_target)\n",
    "    nsdr[i-100,1] = get_NSDR(sdr[i-100,1],audio,acc_target)\n",
    "print('terminei! ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results: ')\n",
    "print('vocal mean SDR: ', np.mean(sdr[:,0]))\n",
    "print('vocal mean NSDR: ', np.mean(nsdr[:,0]))\n",
    "print('vocal mean SIR: ', np.mean(sir[:,0]))\n",
    "print('vocal mean SAR: ', np.mean(sar[:,0]))\n",
    "print('acc mean SDR: ', np.mean(sdr[:,1]))\n",
    "print('acc mean NSDR: ', np.mean(nsdr[:,1]))\n",
    "print('acc mean SIR: ', np.mean(sir[:,1]))\n",
    "print('acc mean SAR: ', np.mean(sar[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_umx(input_spec,freq_bins,sample_len):\n",
    "    DROPOUT = 0.4\n",
    "    hidden_size = 512\n",
    "    print(input_spec.shape)\n",
    "    l1 = K.permute_dimensions(input_spec,(0,2,1,3))\n",
    "    l1 = K.reshape(l1,(-1,freq_bins))\n",
    "    #l1 = input_spec\n",
    "    l2 = Dense(hidden_size)(l1)\n",
    "    l2 = BatchNormalization()(l2)\n",
    "    l2 = K.reshape(l2,(128,-1,hidden_size))\n",
    "\n",
    "\n",
    "    l3 = Activation('tanh')(l2)\n",
    "    l4 = K.reshape(l3,(sample_len,-1,hidden_size))\n",
    "    #l4 = K.expand_dims(l4,axis=-1)\n",
    "    x1 = Bidirectional(LSTM(int(hidden_size/2), return_sequences=True,dropout=DROPOUT))(l4)\n",
    "\n",
    "    #x1 = Dropout(0.4)(x1)\n",
    "\n",
    "    x2 = Bidirectional(LSTM(int(hidden_size/2), return_sequences=True,dropout=DROPOUT))(x1)\n",
    "    #x2 = Dropout(0.4)(x2)\n",
    "    \n",
    "    x3 = Bidirectional(LSTM(int(hidden_size/2), return_sequences=True,dropout=DROPOUT))(x2)\n",
    "    #x3 = Dropout(0.4)(x3)\n",
    "    \n",
    "    \n",
    "    #x3 = tf.keras.layers.TimeDistributed(Dense(freq_bins))(x3)\n",
    "    \n",
    "    \n",
    "    c1 = concatenate([l4,x3],axis=2)\n",
    "    c1 = tf.reshape(c1,(-1,c1.shape[-1]))\n",
    "    c2 = Dense(hidden_size)(c1)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c3 = Activation('relu')(c2)\n",
    "    c4 = Dense(freq_bins)(c3)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c5 = Activation('relu')(c4)\n",
    "    #c6 = K.permute_dimensions(c5,(1,0))\n",
    "    c6 = K.reshape(c5,(-1,sample_len,freq_bins))\n",
    "    c6 = K.permute_dimensions(c6,(0,2,1))\n",
    "    c6 = K.reshape(c6,(-1,freq_bins,sample_len,1))\n",
    "    return c6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for name in tqdm(names):\n",
    "    basename = name.rsplit(\"_\",1)[0]\n",
    "    sample_no = name.rsplit(\"_\",2)[1]\n",
    "    src_voc = basename + '_vocals.wav'\n",
    "    src_mix = basename + '_mix.wav'\n",
    "    final_path_voc = sample_no + '/' + 'vocals.wav'\n",
    "    final_path_mix = sample_no + '/' + 'mix.wav'\n",
    "    !mkdir $sample_no\n",
    "    !mv $src_voc $final_path_voc\n",
    "    !mv $src_mix $final_path_mix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev_pedro] *",
   "language": "python",
   "name": "conda-env-dev_pedro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
